{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RyanZurrin/ComputerVision/blob/main/MODNet_WebCam_Based_Video_Matting_Demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "he2gSkd1_Hab"
      },
      "source": [
        "# MODNet - WebCam-Based Portrait Video Matting Demo\n",
        "\n",
        "[![arXiv](https://img.shields.io/badge/arXiv-Paper-<COLOR>.svg)](https://arxiv.org/pdf/2011.11961.pdf)          [![GitHub stars](https://img.shields.io/github/stars/ZHKKKe/MODNet?style=social)](https://github.com/ZHKKKe/MODNet)\n",
        "\n",
        "<p align=\"justify\">This is a <b>WebCam-Based Portrait Video Matting Demo</b> of our paper ''<a href=\"https://arxiv.org/pdf/2011.11961.pdf\">Is a Green Screen Really Necessary for Real-Time Portrait Matting?</a>''. We propose a trimap-free MODNet for portrait matting in real time (on a single GPU). If you want to perform matting for portrait images, please refer to our <b>Portrait Image Matting Demo</b> [<a href=\"https://colab.research.google.com/drive/1GANpbKT06aEFiW-Ssx0DQnnEADcXwQG6?usp=sharing\">colab</a>].\n",
        "\n",
        "<!-- <img src=\"https://raw.githubusercontent.com/ZHKKKe/MODNet/develop/doc/gif/image_matting_demo.gif\" width=\"40%\"> -->\n",
        "\n",
        "<p align=\"justify\"> We use ~400 unlabeled video clips (divided into ~50,000 unlabeled frames) downloaded from the internet to perform SOC to adapt MODNet to the video domain. Nonetheless, <b><font color='#FF000'>due to insufficient labeled training data (~3k labeled foregrounds), our model may still make errors in portrait semantics estimation under challenging scenes</font></b>.\n",
        "\n",
        "Please run the code step by step to call your WebCam for real-time portrait video matting. \n",
        "Note that <b><font color='#FF000'>due to Colab resource limitations, the fps of this demo is very low</font></b> (It is also based on your network status). If you have an Ubuntu system with WebCam, please consider trying our <a href=\"https://github.com/ZHKKKe/MODNet/tree/master/demo/video_matting\">offline demo</a> to get a higher <i>fps</i>. \n",
        "\n",
        "For a better experience, please:\n",
        "\n",
        "*   use a supported browser (Google Chrome is recommended) and allow third-party cookies (used to call WebCam)\n",
        "*   make sure the network is in good condition\n",
        "*   make sure the portrait and background are distinguishable, <i>i.e.</i>, are not similar\n",
        "*   run in soft and bright ambient lighting\n",
        "*   do not be too close or too far from the WebCam\n",
        "*   do not move too fast\n",
        "\n",
        "By the way, if you have any suggestions on improving the efficiency of executing JS scripts in Colab, <i>i.e.</i>, to make this demo have a higher <i>fps</i>, please let me know. Thanks in advance!\n",
        "</p>\n",
        "\n",
        "### **Let's start!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4qPYwklGk2b"
      },
      "source": [
        "## 1. Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEeBUunUHAnp"
      },
      "source": [
        "<p align=\"justify\">In the top menu of this session, select <b>Runtime -> Change runtime type</b>, and set <b>Hardware Accelerator</b> to <b>GPU</b>.</p>\n",
        "\n",
        "<p align=\"justify\">Clone the repository, and download the pre-trained model:</p>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQuJjy_UKpvW",
        "outputId": "1d22b143-da63-40b6-d91d-c1cf7eb27b8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'MODNet'...\n",
            "remote: Enumerating objects: 270, done.\u001b[K\n",
            "remote: Counting objects: 100% (64/64), done.\u001b[K\n",
            "remote: Compressing objects: 100% (50/50), done.\u001b[K\n",
            "remote: Total 270 (delta 31), reused 30 (delta 11), pack-reused 206\u001b[K\n",
            "Receiving objects: 100% (270/270), 60.77 MiB | 14.04 MiB/s, done.\n",
            "Resolving deltas: 100% (92/92), done.\n",
            "/content/MODNet\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Nf1ZxeJZJL8Qx9KadcYYyEmmlKhTADxX\n",
            "To: /content/MODNet/pretrained/modnet_webcam_portrait_matting.ckpt\n",
            "100% 26.3M/26.3M [00:00<00:00, 55.6MB/s]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# clone the repository\n",
        "%cd /content\n",
        "if not os.path.exists('MODNet'):\n",
        "  !git clone https://github.com/ZHKKKe/MODNet\n",
        "%cd MODNet/\n",
        "\n",
        "# dowload the pre-trained ckpt for video matting\n",
        "pretrained_ckpt = 'pretrained/modnet_webcam_portrait_matting.ckpt'\n",
        "if not os.path.exists(pretrained_ckpt):\n",
        "  !gdown --id 1Nf1ZxeJZJL8Qx9KadcYYyEmmlKhTADxX \\\n",
        "          -O pretrained/modnet_webcam_portrait_matting.ckpt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GftrpmnoHGCV"
      },
      "source": [
        "Define JS script to call WebCam:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "prBcl0v0KvR2"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import base64\n",
        "from google.colab.output import eval_js\n",
        "from IPython.display import display, Javascript\n",
        "\n",
        "\n",
        "def prepare_webcam():\n",
        "  display(\n",
        "    Javascript('''\n",
        "      var div = null;\n",
        "      var video;\n",
        "      var stream;\n",
        "      var imgElement;\n",
        "      var captureCanvas;\n",
        "      var pendingResolve = null;\n",
        "      \n",
        "      async function onAnimationFrame() {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      \n",
        "        if (pendingResolve) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
        "          var result = captureCanvas.toDataURL('image/jpeg', 0.8);\n",
        "\n",
        "          var lp = pendingResolve;\n",
        "          pendingResolve = null;\n",
        "          lp(result);\n",
        "        }\n",
        "      }\n",
        "      \n",
        "      async function initWebCam() {\n",
        "        div = document.createElement('div');\n",
        "        document.body.appendChild(div);\n",
        "            \n",
        "        video = document.createElement('video');\n",
        "        video.style.display = 'inline-block';\n",
        "        stream = await navigator.mediaDevices.getUserMedia(\n",
        "            {video: { facingMode: \"environment\"}});\n",
        "        div.appendChild(video);\n",
        "\n",
        "        imgElement = document.createElement('img');\n",
        "        imgElement.style.display = 'inline-block';\n",
        "        div.appendChild(imgElement);\n",
        "        \n",
        "        video.srcObject = stream;\n",
        "        await video.play();\n",
        "\n",
        "        captureCanvas = document.createElement('canvas');\n",
        "        captureCanvas.width = 640;\n",
        "        captureCanvas.height = 480;\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "        \n",
        "        return stream;\n",
        "      }\n",
        "\n",
        "      async function processFrame(fgFrame) {\n",
        "        if (div == null) {\n",
        "          stream = await initWebCam();\n",
        "        }\n",
        "\n",
        "        if (fgFrame != \"\") {\n",
        "          var videoRect = video.getClientRects()[0];\n",
        "          imgElement.style.top = videoRect.top + \"px\";\n",
        "          imgElement.style.left = videoRect.left + \"px\";\n",
        "          imgElement.style.width = videoRect.width + \"px\";\n",
        "          imgElement.style.height = videoRect.height + \"px\";\n",
        "          imgElement.src = fgFrame;\n",
        "        }\n",
        "\n",
        "        return await new Promise(\n",
        "          function(resolve, reject) {pendingResolve = resolve;});\n",
        "      }\n",
        "    ''')\n",
        "  )\n",
        "    \n",
        "def process_frame(fg_frame):\n",
        "  return eval_js('processFrame(\"{}\")'.format(fg_frame))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nze8eWbfHaoF"
      },
      "source": [
        "Load the pre-trained MODNet and define the inference code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "8pmCZ-kt4dU-"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import PIL\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from src.models.modnet import MODNet\n",
        "\n",
        "\n",
        "modnet = MODNet(backbone_pretrained=False)\n",
        "modnet = nn.DataParallel(modnet).cuda()\n",
        "modnet.load_state_dict(torch.load(pretrained_ckpt))\n",
        "modnet.eval()\n",
        "\n",
        "\n",
        "torch_transforms = transforms.Compose(\n",
        "  [\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "  ]\n",
        ")\n",
        "\n",
        "\n",
        "def modnet_matting(modnet, im_frame):\n",
        "  im_bytes = base64.b64decode(im_frame.split(',')[1])\n",
        "  im_PIL = PIL.Image.open(io.BytesIO(im_bytes))\n",
        "  im_np = np.asarray(im_PIL)\n",
        "\n",
        "  im_tensor = torch_transforms(im_PIL)\n",
        "  im_tensor = im_tensor[None, :, :, :].cuda()\n",
        "  \n",
        "  _, _, matte_tensor = modnet(im_tensor, True)\n",
        "  matte_tensor = matte_tensor.repeat(1, 3, 1, 1)\n",
        "  matte_np = matte_tensor[0].data.cpu().numpy().transpose(1, 2, 0)\n",
        "  fg_np = matte_np * im_np + (1 - matte_np) * np.full(im_np.shape, 255.0)\n",
        "  fg_PIL = PIL.Image.fromarray(np.uint8(fg_np))\n",
        "\n",
        "  io_buffer = io.BytesIO()\n",
        "  fg_PIL.save(io_buffer, format='jpeg')\n",
        "  fg_frame = 'data:image/jpeg;base64,{}'.format(\n",
        "      (str(base64.b64encode(io_buffer.getvalue()), 'utf-8')))\n",
        "  return fg_frame\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdYVB5CiJnS7"
      },
      "source": [
        "## 2. Run Video Demo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GJ3B7Wbv4f_x",
        "outputId": "8c879095-f851-4897-a5d1-80a29f78ad92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "      var div = null;\n",
              "      var video;\n",
              "      var stream;\n",
              "      var imgElement;\n",
              "      var captureCanvas;\n",
              "      var pendingResolve = null;\n",
              "      \n",
              "      async function onAnimationFrame() {\n",
              "        window.requestAnimationFrame(onAnimationFrame);\n",
              "      \n",
              "        if (pendingResolve) {\n",
              "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
              "          var result = captureCanvas.toDataURL('image/jpeg', 0.8);\n",
              "\n",
              "          var lp = pendingResolve;\n",
              "          pendingResolve = null;\n",
              "          lp(result);\n",
              "        }\n",
              "      }\n",
              "      \n",
              "      async function initWebCam() {\n",
              "        div = document.createElement('div');\n",
              "        document.body.appendChild(div);\n",
              "            \n",
              "        video = document.createElement('video');\n",
              "        video.style.display = 'inline-block';\n",
              "        stream = await navigator.mediaDevices.getUserMedia(\n",
              "            {video: { facingMode: \"environment\"}});\n",
              "        div.appendChild(video);\n",
              "\n",
              "        imgElement = document.createElement('img');\n",
              "        imgElement.style.display = 'inline-block';\n",
              "        div.appendChild(imgElement);\n",
              "        \n",
              "        video.srcObject = stream;\n",
              "        await video.play();\n",
              "\n",
              "        captureCanvas = document.createElement('canvas');\n",
              "        captureCanvas.width = 640;\n",
              "        captureCanvas.height = 480;\n",
              "        window.requestAnimationFrame(onAnimationFrame);\n",
              "        \n",
              "        return stream;\n",
              "      }\n",
              "\n",
              "      async function processFrame(fgFrame) {\n",
              "        if (div == null) {\n",
              "          stream = await initWebCam();\n",
              "        }\n",
              "\n",
              "        if (fgFrame != \"\") {\n",
              "          var videoRect = video.getClientRects()[0];\n",
              "          imgElement.style.top = videoRect.top + \"px\";\n",
              "          imgElement.style.left = videoRect.left + \"px\";\n",
              "          imgElement.style.width = videoRect.width + \"px\";\n",
              "          imgElement.style.height = videoRect.height + \"px\";\n",
              "          imgElement.src = fgFrame;\n",
              "        }\n",
              "\n",
              "        return await new Promise(\n",
              "          function(resolve, reject) {pendingResolve = resolve;});\n",
              "      }\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3680: UserWarning: The default behavior for interpolate/upsample with float scale_factor changed in 1.6.0 to align with other frameworks/libraries, and now uses scale_factor directly, instead of relying on the computed output size. If you wish to restore the old behavior, please set recompute_scale_factor=True. See the documentation of nn.Upsample for details. \n",
            "  \"The default behavior for interpolate/upsample with float scale_factor changed \"\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-800720fca489>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;31m# show the processed frame and capture a new frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0;31m# TODO: this step is very slow!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfg_frame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m   \u001b[0;31m# matting by MODNet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;31m# NOTE: matting inference is fast\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-2-61c2d592d191>\u001b[0m in \u001b[0;36mprocess_frame\u001b[0;34m(fg_frame)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprocess_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfg_frame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0meval_js\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'processFrame(\"{}\")'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfg_frame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# prepare WebCam\n",
        "prepare_webcam()\n",
        "\n",
        "# main loop\n",
        "fg_frame = ''\n",
        "while True:\n",
        "  # show the processed frame and capture a new frame\n",
        "  # TODO: this step is very slow!\n",
        "  frame = process_frame(fg_frame)\n",
        "  # matting by MODNet\n",
        "  # NOTE: matting inference is fast\n",
        "  fg_frame = modnet_matting(modnet, frame)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "MODNet - WebCam-Based Video Matting Demo.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}